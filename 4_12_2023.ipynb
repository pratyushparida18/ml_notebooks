{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3c5a8614",
      "metadata": {
        "id": "3c5a8614"
      },
      "source": [
        "#### Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7cc0f9e",
      "metadata": {
        "id": "e7cc0f9e"
      },
      "source": [
        "In Machine Learning and Artificial Intelligence, Perceptron is the most commonly used term for all folks. It is the primary step to learn Machine Learning and Deep Learning technologies, which consists of a set of weights, input values or scores, and a threshold. Perceptron is a building block of an Artificial Neural Network. Initially, in the mid of 19th century, Mr. Frank Rosenblatt invented the Perceptron for performing certain calculations to detect input data capabilities or business intelligence. Perceptron is a linear Machine Learning algorithm used for supervised learning for various binary classifiers. This algorithm enables neurons to learn elements and processes them one by one during preparation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d23dacf6",
      "metadata": {
        "id": "d23dacf6"
      },
      "source": [
        "Perceptron is Machine Learning algorithm for supervised learning of various binary classification tasks. Further, Perceptron is also understood as an Artificial Neuron or neural network unit that helps to detect certain input data computations in business intelligence.\n",
        "\n",
        "Perceptron model is also treated as one of the best and simplest types of Artificial Neural networks. However, it is a supervised learning algorithm of binary classifiers. Hence, we can consider it as a single-layer neural network with four main parameters, i.e., input values, weights and Bias, net sum, and an activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1eb053b",
      "metadata": {
        "id": "b1eb053b"
      },
      "source": [
        "#### What is Binary classifier in Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0364126c",
      "metadata": {
        "id": "0364126c"
      },
      "source": [
        "In Machine Learning, binary classifiers are defined as the function that helps in deciding whether input data can be represented as vectors of numbers and belongs to some specific class.Binary classifiers can be considered as linear classifiers. In simple words, we can understand it as a classification algorithm that can predict linear predictor function in terms of weight and feature vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e880b3f3",
      "metadata": {
        "id": "e880b3f3"
      },
      "source": [
        "![Perceptron.png](attachment:Perceptron.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89c7aa44",
      "metadata": {
        "id": "89c7aa44"
      },
      "source": [
        "#### Basic Components of Perceptron\n",
        "\n",
        "Perceptron is a type of artificial neural network, which is a fundamental concept in machine learning. The basic components of a perceptron are:\n",
        "\n",
        "Input Layer: The input layer consists of one or more input neurons, which receive input signals from the external world or from other layers of the neural network.\n",
        "\n",
        "Weights: Each input neuron is associated with a weight, which represents the strength of the connection between the input neuron and the output neuron.\n",
        "\n",
        "Bias: A bias term is added to the input layer to provide the perceptron with additional flexibility in modeling complex patterns in the input data.\n",
        "\n",
        "Activation Function: The activation function determines the output of the perceptron based on the weighted sum of the inputs and the bias term. Common activation functions used in perceptrons include the step function, sigmoid function, and ReLU function.\n",
        "\n",
        "Output: The output of the perceptron is a single binary value, either 0 or 1, which indicates the class or category to which the input data belongs.\n",
        "\n",
        "Training Algorithm: The perceptron is typically trained using a supervised learning algorithm such as the perceptron learning algorithm or backpropagation. During training, the weights and biases of the perceptron are adjusted to minimize the error between the predicted output and the true output for a given set of training examples.\n",
        "\n",
        "Overall, the perceptron is a simple yet powerful algorithm that can be used to perform binary classification tasks and has paved the way for more complex neural networks used in deep learning today."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20cfa4c0",
      "metadata": {
        "id": "20cfa4c0"
      },
      "source": [
        "#### How does perceptron work"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "129e1d13",
      "metadata": {
        "id": "129e1d13"
      },
      "source": [
        "Step-1\n",
        "\n",
        "In the first step first, multiply all input values with corresponding weight values and then add them to determine the weighted sum. Mathematically, we can calculate the weighted sum as follows:\n",
        "\n",
        "∑wi*xi = x1*w1 + x2*w2 +…wn*xn\n",
        "\n",
        "Add a special term called bias 'b' to this weighted sum to improve the model's performance.\n",
        "∑wi*xi + b\n",
        "\n",
        "\n",
        "Step-2\n",
        "\n",
        "In the second step, an activation function is applied with the above-mentioned weighted sum, which gives us output either in binary form or a continuous value as follows:\n",
        "\n",
        "Y = f(∑wi*xi + b)\n",
        "\n",
        "Learning:\n",
        "The weights of the perceptron are adjusted during a learning phase. The learning algorithm updates the weights based on the error between the predicted output and the true output. One simple learning rule is the perceptron learning rule, which adjusts the weights as follows:\n",
        "\n",
        "wi= wi + learning rate x (target - output) x xi\n",
        "\n",
        "\n",
        "The learning rate is a small positive constant that determines the size of the weight updates. The learning process continues iteratively until the perceptron learns to correctly classify the input patterns."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddd79d86",
      "metadata": {
        "id": "ddd79d86"
      },
      "source": [
        "#### Types of perceptron model\n",
        "\n",
        "Based on the layers, Perceptron models are divided into two types. These are as follows:\n",
        "\n",
        "Single-layer Perceptron Model\n",
        "Multi-layer Perceptron model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dc7193c",
      "metadata": {
        "id": "5dc7193c"
      },
      "source": [
        "Single Layer Perceptron Model:\n",
        "This is one of the easiest Artificial neural networks (ANN) types. A single-layered perceptron model consists feed-forward network and also includes a threshold transfer function inside the model. The main objective of the single-layer perceptron model is to analyze the linearly separable objects with binary outcomes.\n",
        "\n",
        "In a single layer perceptron model, its algorithms do not contain recorded data, so it begins with inconstantly allocated input for weight parameters. Further, it sums up all inputs (weight). After adding all inputs, if the total sum of all inputs is more than a pre-determined value, the model gets activated and shows the output value as +1.\n",
        "\n",
        "If the outcome is same as pre-determined or threshold value, then the performance of this model is stated as satisfied, and weight demand does not change. However, this model consists of a few discrepancies triggered when multiple weight inputs values are fed into the model. Hence, to find desired output and minimize errors, some changes should be necessary for the weights input.\n",
        "\n",
        "\"Single-layer perceptron can learn only linearly separable patterns.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4b92e95",
      "metadata": {
        "id": "e4b92e95"
      },
      "source": [
        "Multi-Layered Perceptron Model:\n",
        "Like a single-layer perceptron model, a multi-layer perceptron model also has the same model structure but has a greater number of hidden layers.\n",
        "\n",
        "The multi-layer perceptron model is also known as the Backpropagation algorithm, which executes in two stages as follows:\n",
        "\n",
        "Forward Stage: Activation functions start from the input layer in the forward stage and terminate on the output layer.\n",
        "Backward Stage: In the backward stage, weight and bias values are modified as per the model's requirement. In this stage, the error between actual output and demanded originated backward on the output layer and ended on the input layer.\n",
        "Hence, a multi-layered perceptron model has considered as multiple artificial neural networks having various layers in which activation function does not remain linear, similar to a single layer perceptron model. Instead of linear, activation function can be executed as sigmoid, TanH, ReLU, etc., for deployment.\n",
        "\n",
        "A multi-layer perceptron model has greater processing power and can process linear and non-linear patterns. Further, it can also implement logic gates such as AND, OR, XOR, NAND, NOT, XNOR, NOR.\n",
        "\n",
        "Advantages of Multi-Layer Perceptron:\n",
        "\n",
        "A multi-layered perceptron model can be used to solve complex non-linear problems.\n",
        "It works well with both small and large input data.\n",
        "It helps us to obtain quick predictions after the training.\n",
        "It helps to obtain the same accuracy ratio with large as well as small data.\n",
        "Disadvantages of Multi-Layer Perceptron:\n",
        "\n",
        "In Multi-layer perceptron, computations are difficult and time-consuming.\n",
        "In multi-layer Perceptron, it is difficult to predict how much the dependent variable affects each independent variable.\n",
        "The model functioning depends on the quality of the training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9fd19dc",
      "metadata": {
        "id": "b9fd19dc"
      },
      "source": [
        "#### Perceptron Function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfb6a20f",
      "metadata": {
        "id": "dfb6a20f"
      },
      "source": [
        "Perceptron function ''f(x)'' can be achieved as output by multiplying the input 'x' with the learned weight coefficient 'w'.\n",
        "\n",
        "Mathematically, we can express it as follows:\n",
        "\n",
        "f(x)=1; if w.x+b>0\n",
        "\n",
        "otherwise, f(x)=0\n",
        "\n",
        "'w' represents real-valued weights vector\n",
        "'b' represents the bias\n",
        "'x' represents a vector of input x values."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b99de58e",
      "metadata": {
        "id": "b99de58e"
      },
      "source": [
        "#### Characteristics of the Perceptron Model\n",
        "\n",
        "The following are the characteristics of a Perceptron Model:\n",
        "\n",
        "It is a machine learning algorithm that uses supervised learning of binary classifiers.\n",
        "\n",
        "In Perceptron, the weight coefficient is automatically learned.\n",
        "\n",
        "Initially, weights are multiplied with input features, and then the decision is made whether the neuron is fired or not.\n",
        "\n",
        "The activation function applies a step rule to check whether the function is more significant than zero.\n",
        "\n",
        "The linear decision boundary is drawn, enabling the distinction between the two linearly separable classes +1 and -1.\n",
        "\n",
        "If the added sum of all input values is more than the threshold value, it must have an output signal; otherwise, no output will be shown."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7734dc6a",
      "metadata": {
        "id": "7734dc6a"
      },
      "source": [
        "#### Limitation of Perceptron Model\n",
        "\n",
        "The following are the limitation of a Perceptron model:\n",
        "\n",
        "The output of a perceptron can only be a binary number (0 or 1) due to the hard-edge transfer function.\n",
        "\n",
        "It can only be used to classify the linearly separable sets of input vectors. If the input vectors are non-linear, it is not easy to classify them correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5192c7a",
      "metadata": {
        "id": "b5192c7a"
      },
      "source": [
        "#### Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4576f2c3",
      "metadata": {
        "id": "4576f2c3"
      },
      "source": [
        "Activation functions are mathematical equations that determine the output of a neural network model. Activation functions also have a major effect on the neural network’s ability to converge and the convergence speed, or in some cases, activation functions might prevent neural networks from converging in the first place. Activation function also helps to normalize the output of any input in the range between 1 to -1 or 0 to 1.\n",
        "\n",
        "Activation function must be efficient and it should reduce the computation time because the neural network sometimes trained on millions of data points.\n",
        "\n",
        "Let’s consider the simple neural network model without any hidden layers.\n",
        "\n",
        "Here is the output-\n",
        "\n",
        "Y =  ∑ (weights*input + bias)\n",
        "\n",
        "and it can range from -infinity to +infinity. So it is necessary to bound the output to get the desired prediction or generalized results.\n",
        "\n",
        "Y = Activation function(∑ (weights*input + bias))\n",
        "\n",
        "So the activation function is an important part of an artificial neural network. They decide whether a neuron should be activated or not and it is a non-linear transformation that can be done on the input before sending it to the next layer of neurons or finalizing the output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aafde9c",
      "metadata": {
        "id": "8aafde9c"
      },
      "source": [
        "##### Linear activation functions:\n",
        "\n",
        "\n",
        "Linear activation functions are simple mathematical functions that compute a linear relationship between the input and output. In the context of neural networks, the term \"linear activation function\" usually refers to an identity activation function, where the output is directly proportional to the input. Mathematically, the linear activation function is represented as:\n",
        "\n",
        "\n",
        "f(x)=x\n",
        "\n",
        "Here, x is the input to the function, and f(x) is the output. The output is equal to the input, so there is no transformation or non-linearity introduced by this activation function.\n",
        "The function is a straight line with a slope of 1, passing through the origin. This means that the output increases or decreases linearly with the input.\n",
        "Linear activation functions are commonly used in the output layer of regression models. In regression tasks, the network is often trained to predict a continuous output, and a linear activation allows the network to output values over a broad range."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cbf83b2",
      "metadata": {
        "id": "4cbf83b2"
      },
      "source": [
        "##### Non Linear Activatin Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71c39b5d",
      "metadata": {
        "id": "71c39b5d"
      },
      "source": [
        "Non-linear activation functions are mathematical functions applied to the output of a neuron or layer in a neural network, introducing non-linearity to the model. Non-linear activation functions are crucial for enabling neural networks to learn and approximate complex, non-linear relationships in data. Here are some common non-linear activation functions:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8ba3f38",
      "metadata": {
        "id": "f8ba3f38"
      },
      "source": [
        "##### Sigmoid\n",
        "\n",
        "Range: (0, 1)\n",
        "Properties:\n",
        "Squashes input values to the range (0, 1), useful for binary classification problems.\n",
        "Output values can be interpreted as probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4139559",
      "metadata": {
        "id": "e4139559"
      },
      "source": [
        "##### Tanh\n",
        "\n",
        "Range: (-1, 1)\n",
        "Properties:\n",
        "Similar to the sigmoid function but with an output range that includes negative values.\n",
        "Zero-centered, which can aid in model training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6953d7ea",
      "metadata": {
        "id": "6953d7ea"
      },
      "source": [
        "##### Leaky Relu\n",
        "\n",
        "Range: (-∞, +∞)\n",
        "Properties:\n",
        "Addresses the dying ReLU problem by allowing a small, non-zero gradient for negative inputs.\n",
        "The parameter\n",
        "α is typically a small constant."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f976a40",
      "metadata": {
        "id": "6f976a40"
      },
      "source": [
        "##### Max Activation Function\n",
        "\n",
        "Range: Varies based on the parameters w1,b1,w2,b2\n",
        "Properties:\n",
        "Generalization of ReLU and Leaky ReLU.\n",
        "Introduces learnable parameters to determine the output of each piecewise linear function.\n",
        "Allows the model to learn more complex activation functions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "001bf114",
      "metadata": {
        "id": "001bf114"
      },
      "source": [
        "##### Exponential Linear Unit\n",
        "\n",
        "Range: (-α, +∞)\n",
        "Properties:\n",
        "Smooth for negative inputs, addressing some of the issues of ReLU.\n",
        "Converges to a non-zero value as x approaches negative infinity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71baac86",
      "metadata": {
        "id": "71baac86"
      },
      "source": [
        "![activation_funcitons2.png](attachment:activation_funcitons2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01355e4b",
      "metadata": {
        "id": "01355e4b"
      },
      "source": [
        "#### Why Tanh suffers through vanishing gradient problems\n",
        "\n",
        "The vanishing gradient problem is a challenge in training deep neural networks, and the use of the hyperbolic tangent (tanh) activation function is one factor that can contribute to this problem. The vanishing gradient problem occurs when the gradients during backpropagation become very small as they are propagated backward through the layers of the network. This can lead to slow or stagnant learning in the early layers of the network and can make training difficult, especially in deep architectures.\n",
        "\n",
        "The issue with the tanh function is that its output is centered around zero, and its derivative (gradient) is maximized at zero. As the input moves away from zero (either toward positive or negative extremes), the function saturates, meaning that the slope of the function approaches zero. When the derivative becomes close to zero, it results in very small gradients during backpropagation, especially for deep networks.\n",
        "\n",
        "In the context of deep learning, when stacking multiple layers with tanh activations, the vanishing gradient problem can cause the gradients to diminish exponentially as they are propagated backward through the layers. This can result in the early layers learning very slowly or not learning at all, hindering the overall training process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7106834",
      "metadata": {
        "id": "a7106834"
      },
      "source": [
        "#### How relu solves the vanishing gradient problem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d365067",
      "metadata": {
        "id": "0d365067"
      },
      "source": [
        "The key characteristics of ReLU that contribute to mitigating the vanishing gradient problem are:\n",
        "\n",
        "Non-Saturation: ReLU is non-saturating in the positive domain. For positive input values, the output is the input itself. This means that the derivative (gradient) of the ReLU function is equal to 1 for positive inputs. Unlike tanh or sigmoid, which saturate for extreme input values, ReLU does not saturate in the positive range.\n",
        "\n",
        "Sparsity: ReLU introduces sparsity in the network because, for negative inputs, the output is zero. This sparsity can lead to more efficient and effective learning by promoting a more selective activation of neurons."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a91f79c",
      "metadata": {
        "id": "5a91f79c"
      },
      "source": [
        "#### What is dying relu problem?\n",
        "\n",
        "The \"dying ReLU\" problem refers to a situation where neurons using the Rectified Linear Unit (ReLU) activation function become inactive (output zero) for all inputs during the training process and do not recover. In other words, these neurons \"die\" and stop learning or contributing to the learning process.\n",
        "\n",
        "The problem typically arises when a large gradient flows through a ReLU neuron during training, causing the weights to update in a way that makes the neuron always output zero. Once a ReLU neuron is in this state, it remains inactive for all subsequent inputs, and its weights are no longer adjusted during training.\n",
        "\n",
        "The main cause of the dying ReLU problem is the nature of the ReLU function itself. For inputs less than zero, the ReLU function outputs zero, effectively \"killing\" the neuron. If a large gradient consistently pushes the weights in a direction that keeps the neuron in the zero-output region, the neuron becomes inactive and contributes nothing to the learning process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cd5b2c2",
      "metadata": {
        "id": "3cd5b2c2"
      },
      "source": [
        "#### How leaky relu solves the dying relu problem\n",
        "\n",
        "The Leaky ReLU activation function is designed to address the \"dying ReLU\" problem by introducing a small, non-zero slope for negative inputs. The standard ReLU activation function sets the output to zero for any negative input, which can lead to neurons becoming inactive if they consistently receive negative inputs during training. In contrast, Leaky ReLU allows a small, non-zero output for negative inputs, preventing neurons from becoming completely inactive.\n",
        "\n",
        "How Leaky ReLU helps solve the dying ReLU problem:\n",
        "\n",
        "Non-Zero Gradient for Negative Inputs: The small, non-zero slope for negative inputs ensures that there is always a gradient, even for negative values. This helps prevent the weights from becoming stuck during training and allows the neuron to learn and adapt.\n",
        "\n",
        "Preventing Neurons from Dying: By allowing a non-zero output for negative inputs, Leaky ReLU prevents neurons from being completely inactive. Even if a neuron consistently receives negative inputs, it can still contribute to the learning process with a small, non-zero output.\n",
        "\n",
        "Increased Robustness: Leaky ReLU introduces a degree of robustness to the model, making it less prone to issues associated with dead neurons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a7fcb1b",
      "metadata": {
        "id": "7a7fcb1b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}